# Monitoring Systems Overview
This document outlines the components, concepts, and scope of the alerting and monitoring technologies used to track the DIBBs Orchestration Service. While it's not intended to be comprehensive, it should provide a high-level contextual overview of how and why various monitoring pieces are working together. For clarity of organization, explanations of all technical words have been collected into a single section so as to not clutter other paragraphs.

## Packages
* [**OpenTelemetry**](https://opentelemetry.io/docs/languages/python/): OpenTelemetry is one of the most prominent open-source monitoring and logging packages used today. It provides all the infrastructure needed to instrument, collect, and export telemetry data (which consists of traces, metrics, spans, and logs) to other services using the OpenTelemetry Protocol (or OTLP). It offers a staggering range of configuration options, which all need to be fine-tuned together to make telemetry systems work.
* [**Prometheus**](https://prometheus.io/docs/prometheus/latest/getting_started/): Prometheus is an extremely popular tool for application logging and monitoring. Prometheus specifically excels in time-series data point metrics aggregations, typically about system-wide performance or application health  (e.g. CPU usage, number of requests broken up by status code, etc.). Prometheus is what's called a "pull-based" retrieval system, meaning that it should be configured to scrape other API endpoints to fetch metrics data from them, rather than having data sent directly to it. It stores any metrics data it scrapes in a locally mounted storage volume.
* [**Grafana**](https://grafana.com/docs/grafana/latest/): The third element of the OTel / Prometheus ecosystem, Grafana is a visualization system that can display telemetry data in a variety of charts and dashboards. It is the most popular choice to back-end Prometheus, since a Prometheus storage volume can be specified as a Grafana-native data source and metrics loaded directly using the Prometheus Query  Language (PromQL).

## What Are The Pieces: Concepts, Components, and Terms

### Types of Telemetry
* _Traces_: Traces are "residues" or "footprints" that an API request leaves when it enters a system. When the request accesses its entrypoint, or is passed or sent to another service, it leaves a digital record of its processing time at each service along the way. Traces are used to track the information flow of a request in a distributed system.
* _Metrics_: Metrics are aggregations of counts over time (note: _all_ OpenTelemetry metrics are time-series by construction). While Traces track incoming or transported requests, Metrics measure system or request characteristics like latency, CPU load, time-to-request-completion, number of requests received, etc. **DIBBs is primarily interested in metrics monitoring.**
* _Spans_: Spans are sub-components of traces that capture the attributes, or "state", of an incoming request at a particular moment in time. Spans can be a  deep rabbit hole of key-value attribute pairs that can be tuned to track just about any kind of distributed information desired, but for DIBBs purposes, they don't have much relevance.
* _Logs_: Logs are the most self-explanatory kind of telemetry data, the usual fare of print and debug statements a service emits to record that an event has happened. 

### OpenTelemetry Basics and Configuration
* _Instrumentation_: Instrumentation refers to the process of setting up a service to emit telemetry data (note that instrumentation refers only to _producing_ telemetry data, not doing anything with it). Instrumentation can be either manual or automatic (there's also a third option, programmatic, but for our purposes we won't be using that at all), or some combination of both.
* _Automatic Instrumentation_: This type of instrumentation allows a bootstrapped distribution of OpenTelemetry to scan a service's codebase and, on the back end, add in all the infrastructure context for producing telemetry data. Telemetry producers and consumers are created, combined, and wired together with a global context to make each API endpoint of a service emit a suite of standard information (including traces for all incoming requests, spans for request attributes, and metrics for application health and system performance).
* _Manual Instrumentation_: This type of instrumentataion must be configured and implemented by a developer. In any case where you'd like to collect and emit information that isn't automatically collected--or you'd like to customize how a particular type of information is emitted--manual instrumentation is required. To instrument a piece of an application, you need three things: a Provider, an Agent, and an Instrument. These things aren't hard to set up, but do require invoking in the correct order. 
* _Instrumentation Provider_: A global, module- or application-level information context that ensures telemetry is produced in the same place for the same file. A Provider also serves as the API gateway for developers to access OpenTelemetry functions. **For DIBBs, Automatic Instrumentation has already configured all module providers for us.**
* _Instrumentation Agent_: A module-level context responsible for managing everything that one part of the service will emit. Agents create and manage the individual instruments that collect information.
* _Instrumentation Instrument_: An instrument is a single piece of reportability code that tracks and emits data on one thing. A metric that counts the number of requests to a single endpoint is an instrument, for example.

### The OTel Collector
The OpenTelemetry Collector is a series of components that sits between the instrumented service and the storage/query/visualization backend (in our case, Prometheus, since Grafana networks with Prometheus rather than Orchestration directly). It's actually a pipeline through which telemetry data passes, and it's made up of three components:
* _Receiver_: A Collector's Receiver is the component responsible for gathering telemetry data produced by other parts of the system. A Receiver "listens" on a certain port for packeted telemetry sent to it by the Instrumentation Context, and when it detects telemetry packets on that port, it decodes them into an internal representation called OTLP (which is a specified machine protocol that telemetry data must comply with).
* _Processor_: A Collector's Processor is the component responsible for applying batch processing or filtering to the received data. For our purposes, we don't execute any steps here, so Processor is a no-op.
* _Exporter_ The Exporter is the component that actually sends OTLP-formatted data to the back-end. It is the most important part of the Collector since that's how telemetry actually reaches its end state. For DIBBs purposes, the Exporter is actually a little bit misleading: the OTel Collector, insteading of actually sending any data itself, _exposes_ the information it's processed on a `/metrics` endpoint of its own hosted namespace. This allows a service like Prometheus to scrape that information at its own specified interval.

#### Note on where Prometheus fits into a collector
Confusingly, OpenTelemetry allows Prometheus to slot in as either a Receiver _or_ an Exporter in a Collector pipeline. Furthermore, if exporting with Prometheus, OpenTelemetry can be ambiguous about whether Prometheus is a destination that's going to receive data, _or_ if Prometheus is the entity that's going to _collect data from the Collector_ (yes, that is the actual terminology). To break this down, there are three possible cases worth enumerating (so that we can sanity check that future work is still following the pattern we want):

- Prometheus as Receiver: in this case, Prometheus is looking for telemetry data that's been instrumented in a system to be sent directly to Prometheus. The role of the Collector in this configuration is that OpenTelemetry is then going to collect metrics _about what Prometheus collected_, which is **not** what we want.
- Prometheus as entity being exported to: there's a configuration option allowable in both OpenTelemetry and Prometheus called `PrometheusRemoteWrite` that allows OpenTelemetry to send metrics directly to Prometheus' local backend storage via data push. While this technically gets data into Prometheus, it is **not** the system we want to use. Prometheus is optimized for pull-based data retrieval, so having OpenTelemetry use a push operation to just write directly to storage will disable some important time-series aggregation functionality that Prometheus provides.
- Prometheus as entity collecting from the OTel Collector: **this is the option we want to be using**. After shipping off formatted telemetry data to the console, the OTel Collector will make available all the metrics it processed at an endpoint local to its own service host. Exposing the metrics at this endpoint will allow the Prometheus periodic job configuration to query them at its own defined interval while still optimizing for pull-based efficiency.

## How We've Set Things Up
* We start by using OpenTelemetry's automatic instrumentation to create all of the Provider-level instrumentation contexts for the Orchestration Service. This is executed in the project's `Dockerfile`.
* We supplement the automatic instrumentation with manual instrumentation around specific status code formatting for message processing endpoints. This is instrumented in `main.py`.
* We spin up local instances of the OTel Collector, Prometheus, and Grafana, making sure to expose specific relative ports and endpoints. This is executed in the project's `docker-compose.yml`.
* We configure the OTel Collector with the listening/transmitting mechanisms we want to use, and then activate the Collector with a service pipeline. This is set up in `otel-collector-config.yaml`.
* We configure Prometheus with a scrape job to fetch the metrics that the OTel Collector put together for us based on our instrumentation. This is configured in `prometheus.yml`.
* We link Prometheus to Grafana as its database source so that it can pull metrics from Prometheus' local DB into visualization dashboards (TODO: ADD MORE INFO ON THIS ONCE WE DO IT).

## The Monitoring Flow This Creates
When an API request hits the `process-message` endpoint of the Orchestration Service, it goes through the following process:
* The Instrumentation Provider for the Orchestration Service is activated (since Providers are module-level, and endpoints are all in the `main.py` folder, there is one Metrics Provider for the whole service), letting all the telemetry producers that are automatically or manually instrumented know that they have a job to do.
* The HTTP Request leaves a trace in the `process-message` endpoint, which is stored for the moment.
* The Request gets bounced around the various DIBBs services, emitting a trace for each service it passes through. These traces are used to calculate standard, automatically provided backend metrics like latency, time to completion, etc.
* The `process-message` function has finished calling all other DIBBs service and is about to return. When the Return value is created, before it's actually handed to the caller, a final trace is emitted. This trace is used in conjunction with the initial trace the request first created to calculate the time to completion of this request.
* Additionally, the status code that the `building_blocks_response` returned is copied by the Meter created at the top of the file. This Meter hands the status code to the metric responsible for counting calls to the `process-message` endpoint, which then uses a dictionary index to increment the number of calls that returned the particular status code by 1.
* The request is finished, computation has generated a Return value, and this is sent back to the user.
* After the return value is dispatched (we don't want the caller waiting on metrics calculation to get their answer), the Meter Provider sends all traces, spans, logs, and metrics generated or affected by this particular request to the OTLP export endpoint specified in our environment variables--in this case, `otel-collector:4318`.
* The Collector's OTLP Receiver detects new information sent to one of its ports, so it retrieves that information and encodes it into OTLP standard.
* The Collector's Processor has no defined filtering or functionality, so it batches all this data together and passes it on.
* The Collector's Exporter first dumps a decoded copy of the telemetry data (from OTLP to human-readable) to the console for debugging.
* Then, the Collector sends the original, still-encoded data to an endpoint at one of its own ports, namely `otel-collector:8889/metrics`.
* Any and all data that's been sent to `/metrics` without the `/metrics` endpoint being hit continues to remain there until the Prometheus `scrape_interval` hits.
* The local Prometheus server makes an API call to the Collector's `8889/metrics` endpoint, scraping the still-OTLP encoded data into its own local, in-container database.
* Prometheus data volumes are OTLP compatible, so the data doesn't need to be further encoded or modified before it's finally transferred to its end-destination mounted storage volume.
* TODO: ADD IN GRAFANA STEPS ONCE WE KNOW THEM

## Where This All Happens In The Files

### `Dockerfile`
We `pip install` and then execute a bootstrap command for an OpenTelemetry distro package. We also define some environment variables that are needed for the OTel collector to function properly:
- Each `OTEL_[TRACES/METRICS/LOGS]_EXPORTER` variable tells the Instrumentation Context what pathway to use for that typye of telemetry data. `None` means we don't want to see that data type at all, `Console` means print it to `STDOUT`, and `otlp` means use the OTLP protocol defined with the OTel Collector.
- `OTEL_EXPORTER_OTLP_PROTOCOL` tells the Instrumentation Context how to send telemetry data so that the OTel Collector can understand it. By default, OpenTelemetry wires things over `gRPC`, but Python developer SDK notes strongly encourage `http/protobuf`.
- `OTEL_EXPORTER_OTLP_ENDPOINT` tells the Instrumentation Context where to send telemetry so that the Collector can pick it up. The default port for `http/protobuf` is 4318, but note that the host is the `otel-collector` rather than the localhost container in which the entire service is running.

### `main.py`
The top of the code shows a bit of manual instrumentation to create metrics for each endpoint that might be hit with a message processing request. Note that we don't need to create a Provider, since auto instrumentation does that for us, but we do still need to create a Meter (which is an Agent in the pattern) and then a Metric (which is an instrument).

### `docker-compose.yml`
The project's docker compose file is the heart of the connections needed to successfully emit, collect, and report telemetry data. In addition to spinning up the DIBBs services that Orchestration relies on, we also instantiate three other services needed for telemetry reporting:

- The OTel Collector: The Collector is its own service (since it is essentially a small data pipeline) and must be configured as such. Some things worth noting:
    * We use the `-contrib` image of the Collector because it offers a wider suite of parameterization and tuning options; the ordinary `opentelemetry-collector` image is extremely limited and not recommended for cloud use.
    * The `container_name` parameter is **critical**: since the Collector will be its own service, it _will not_ exist in the local namespace of the Orchestration Container or of Prometheus. Using the `container_name` parameter will allow us to specify the ports _hosted by the Collector_ as the ones we should be scraping for data (Docker will resolve this name for us into the Container's individual IP).
    * We need to mount the `otel-collector-config.yaml` file in the project directory into the appropriate directory in the Collector container, and then execute a config command once there.
    * We need to expose port 4317 (for g`RPC` export protocol, even though we don't send via it), port 4318 (the default `http/protobuf` destination), port 8888 (for metrics available about the collector's performance), and port 8889 (where we'll expose the actual metrics that come from the Orchestration service; this is the port Prometheus will scrape).
- Prometheus: Nothing too special here, just using the latest image of prometheus in conjunction with the same kind of mounted volume configuration that we did with the collector. The only thing of note is that if we want metrics storage to persist between container runs, we need to mount the storagae volume by defining `prom_data` explicitly.
- Grafana: Again, nothing out of the ordinary, just using some environment variables to securely authenticate.

### `otel-collector-config.yaml`
This file defines the configuration parameters for the OTel Collector we'll instantiate. The `http` and `grpc` port endpoints are the default values recommended with OpenTelemetry, but note that we preface the host as `0.0.0.0` because this configuration file is from the point of view of the Collector--these ports are local to it, so it can use its own hosted namespace rather than needing `otel-collector` like external services will. We want only the ordinary `otlp` receiver collecting information, have no particular filtering or processing operations, and want to send exported data to both the console (which is what `debug` means) and Prometheus. To get it into Prometheus, we _don't_ push it via `remotewrite`, but instead expose an endpoint local to the Collector that Prometheus will periodically scrape. The `service` section of the config file actually activates all the components we defined, and specifies how each type of telemetry will be processed.

### `prometheus.yml`
A simple specification that tells Prometheus how to collect the metrics data. All prometheus config files must start with the `global` and `scrape_interval` header, and then feature a `scrape_config` section that defines the jobs Prometheus will perform. The Orchestration Monitor has a single job, which is to aggregate telemetry metrics from the OTel Collector, so we provide port 8889 on host `otel-collector` as the target to get those metrics.